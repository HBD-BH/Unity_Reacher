{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the second project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Reacher.app\"`\n",
    "- **Windows** (x86): `\"path/to/Reacher_Windows_x86/Reacher.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Reacher_Windows_x86_64/Reacher.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Reacher_Linux/Reacher.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Reacher_Linux/Reacher.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Reacher_Linux_NoVis/Reacher.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Reacher.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Reacher.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 1\n",
      "Size of each action: 4\n",
      "There are 1 agents. Each observes a state with length: 33\n",
      "The state for the first agent looks like: [ 0.00000000e+00 -4.00000000e+00  0.00000000e+00  1.00000000e+00\n",
      " -0.00000000e+00 -0.00000000e+00 -4.37113883e-08  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00 -1.00000000e+01  0.00000000e+00\n",
      "  1.00000000e+00 -0.00000000e+00 -0.00000000e+00 -4.37113883e-08\n",
      "  0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "  0.00000000e+00  0.00000000e+00  5.75471878e+00 -1.00000000e+00\n",
      "  5.55726671e+00  0.00000000e+00  1.00000000e+00  0.00000000e+00\n",
      " -1.68164849e-01]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents\n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agent's performance, if it selects an action at random with each time step.  A window should pop up that allows you to observe the agent, as it moves through the environment.  \n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agent is able to use its experience to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "while True:\n",
    "    actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "    actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "    env_info = env.step(actions)[brain_name]           # send all actions to the environment\n",
    "    next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "    rewards = env_info.rewards                         # get reward (for each agent)\n",
    "    dones = env_info.local_done                        # see if episode finished\n",
    "    scores += env_info.rewards                         # update the score (for each agent)\n",
    "    states = next_states                               # roll over states to next time step\n",
    "    if np.any(dones):                                  # exit loop if episode finished\n",
    "        break\n",
    "print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training\n",
    "\n",
    "Let's set up our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\tgoal_speed -> 1.0\n",
      "\t\tgoal_size -> 5.0\n",
      "Unity brain name: ReacherBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 33\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "env = UnityEnvironment(file_name='Reacher_Linux/Reacher.x86_64')\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "num_agents = len(env_info.agents)\n",
    "action_size = brain.vector_action_space_size\n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import Agent\n",
    "agent = Agent(state_size, action_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([[  0.0000,  -4.0000,   0.0000,   1.0000,  -0.0000,  -0.0000,\n",
      "          -0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000, -10.0000,   0.0000,   1.0000,  -0.0000,\n",
      "          -0.0000,  -0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
      "           0.0000,   0.0000,  -6.3891,  -1.0000,   4.8145,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.67622364 -0.84744024 -1.         -0.04106396]]\n",
      "State: tensor([[-0.0706, -3.9991, -0.0570,  0.9999, -0.0088, -0.0001, -0.0071,\n",
      "          0.2819,  0.0030, -0.3499, -1.4064,  0.0245, -1.1329, -0.0570,\n",
      "         -9.9957,  0.0322,  0.9997,  0.0110, -0.0006,  0.0220, -0.8758,\n",
      "          0.0361,  0.4388, -0.4977,  0.1720,  1.9197, -6.4216, -1.0000,\n",
      "          4.7710,  0.0000,  1.0000,  0.0000, -0.0973]])\n",
      "Action: [[-0.78335357 -0.84664655 -1.         -0.14739197]]\n",
      "State: tensor([[-0.2353, -3.9893, -0.1860,  0.9993, -0.0293, -0.0007, -0.0232,\n",
      "          0.4672,  0.0225, -0.6056, -2.4337,  0.1828, -1.8707, -0.2007,\n",
      "         -9.9424,  0.1266,  0.9965,  0.0346, -0.0081,  0.0757, -1.6189,\n",
      "          0.2720,  0.6623, -1.1795,  1.3424,  3.8177, -6.4539, -1.0000,\n",
      "          4.7273,  0.0000,  1.0000,  0.0000, -0.0973]])\n",
      "Action: [[-0.9929566  -0.9010244  -1.         -0.17109132]]\n",
      "State: tensor([[-0.4691, -3.9598, -0.3263,  0.9975, -0.0584, -0.0024, -0.0408,\n",
      "          0.4287,  0.0591, -0.8024, -3.2184,  0.4514, -1.6864, -0.4238,\n",
      "         -9.7741,  0.3126,  0.9861,  0.0621, -0.0315,  0.1508, -2.0826,\n",
      "          0.7110,  0.6672, -1.9531,  3.5093,  5.6011, -6.4858, -1.0000,\n",
      "          4.6834,  0.0000,  1.0000,  0.0000, -0.0973]])\n",
      "Action: [[-1.         -0.91756994 -1.         -0.05706769]]\n",
      "State: tensor([[-0.7614, -3.9064, -0.4131,  0.9941, -0.0950, -0.0050, -0.0521,\n",
      "          0.2225,  0.1006, -0.9936, -3.9570,  0.7677, -0.8082, -0.7166,\n",
      "         -9.4578,  0.6140,  0.9645,  0.0873, -0.0767,  0.2371, -2.3915,\n",
      "          1.2671,  0.5133, -2.5609,  6.1073,  7.0818, -6.5175, -1.0000,\n",
      "          4.6392,  0.0000,  1.0000,  0.0000, -0.0973]])\n",
      "Action: [[-1.         -0.89275867 -1.          0.08470762]]\n",
      "State: tensor([[-1.0949, -3.8275, -0.4076,  0.9891, -0.1373, -0.0073, -0.0522,\n",
      "         -0.0671,  0.1178, -1.0909, -4.2703,  1.0744,  0.3788, -1.0539,\n",
      "         -8.9886,  1.0357,  0.9274,  0.1032, -0.1450,  0.3292, -2.7180,\n",
      "          1.7334,  0.1199, -2.7916,  8.6676,  8.2223, -6.5488, -1.0000,\n",
      "          4.5949,  0.0000,  1.0000,  0.0000, -0.0973]])\n",
      "Action: [[-1.         -0.8810139  -1.          0.25957787]]\n",
      "State: tensor([[ -1.4231,  -3.7281,  -0.3077,   0.9829,  -0.1797,  -0.0074,\n",
      "          -0.0403,  -0.3716,   0.0957,  -1.0587,  -4.0287,   1.2958,\n",
      "           1.5313,  -1.3841,  -8.3774,   1.5587,   0.8695,   0.0996,\n",
      "          -0.2284,   0.4265,  -3.1522,   1.8902,  -0.3986,  -2.5505,\n",
      "          11.0684,   8.9727,  -6.5799,  -1.0000,   4.5503,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.        -0.8717604 -1.         0.4054556]]\n",
      "State: tensor([[ -1.7119,  -3.6166,  -0.1173,   0.9758,  -0.2181,  -0.0036,\n",
      "          -0.0158,  -0.7023,   0.0440,  -0.9277,  -3.4079,   1.4043,\n",
      "           2.6465,  -1.6686,  -7.6364,   2.1597,   0.7877,   0.0719,\n",
      "          -0.3113,   0.5268,  -3.6062,   1.6337,  -0.7722,  -2.0291,\n",
      "          13.3289,   9.3189,  -6.6106,  -1.0000,   4.5055,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.86375093 -1.          0.51086164]]\n",
      "State: tensor([[ -1.9343,  -3.5016,   0.1631,   0.9684,  -0.2483,   0.0056,\n",
      "           0.0223,  -1.0712,  -0.0164,  -0.6954,  -2.4704,   1.4019,\n",
      "           3.7721,  -1.8851,  -6.7765,   2.8152,   0.6848,   0.0233,\n",
      "          -0.3725,   0.6259,  -3.9460,   1.0096,  -0.7359,  -1.4690,\n",
      "          15.4870,   9.2228,  -6.6411,  -1.0000,   4.4605,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.8547124  -1.          0.59464526]]\n",
      "State: tensor([[ -2.0655,  -3.3879,   0.5408,   0.9608,  -0.2657,   0.0209,\n",
      "           0.0757,  -1.4921,  -0.0449,  -0.3539,  -1.2359,   1.3738,\n",
      "           5.0361,  -2.0264,  -5.8056,   3.5032,   0.5690,  -0.0365,\n",
      "          -0.3944,   0.7207,  -4.1009,   0.2778,  -0.2012,  -1.1238,\n",
      "          17.5405,   8.6230,  -6.6712,  -1.0000,   4.4153,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.79887044 -1.          0.62715673]]\n",
      "State: tensor([[ -2.0825,  -3.2616,   1.0367,   0.9519,  -0.2658,   0.0410,\n",
      "           0.1468,  -1.9856,   0.0200,   0.0737,   0.2625,   1.6328,\n",
      "           6.6274,  -2.0983,  -4.7245,   4.2062,   0.4475,  -0.0952,\n",
      "          -0.3692,   0.8090,  -4.1106,  -0.2433,   0.6723,  -1.1907,\n",
      "          19.3904,   7.4739,  -6.7010,  -1.0000,   4.3699,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.        -0.7443292 -1.         0.6390053]]\n",
      "State: tensor([[ -1.9658,  -3.0639,   1.6801,   0.9376,  -0.2448,   0.0626,\n",
      "           0.2390,  -2.5821,   0.2454,   0.5129,   1.9826,   2.8532,\n",
      "           8.6157,  -2.1125,  -3.5149,   4.9164,   0.3227,  -0.1411,\n",
      "          -0.2986,   0.8870,  -4.0398,  -0.3659,   1.6260,  -1.6356,\n",
      "          20.9855,   5.9777,  -6.7305,  -1.0000,   4.3243,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.56024355 -1.          0.6295972 ]]\n",
      "State: tensor([[ -1.6787,  -2.6888,   2.4636,   0.9110,  -0.1991,   0.0775,\n",
      "           0.3526,  -3.1530,   0.7515,   0.9396,   4.3525,   5.4562,\n",
      "          10.2413,  -2.0603,  -2.1331,   5.6130,   0.1974,  -0.1606,\n",
      "          -0.1853,   0.9492,  -3.8646,  -0.0382,   2.7328,  -2.2680,\n",
      "          22.2067,   4.0996,  -6.7597,  -1.0000,   4.2785,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.10337764 -1.          0.6436951 ]]\n",
      "State: tensor([[ -1.1708,  -2.0398,   3.2624,   0.8661,  -0.1285,   0.0717,\n",
      "           0.4778,  -3.4510,   1.5400,   1.1283,   7.2712,   9.1073,\n",
      "           9.8088,  -1.9331,  -0.5941,   6.1809,   0.0780,  -0.1349,\n",
      "          -0.0286,   0.9874,  -3.5819,   0.8676,   3.9249,  -3.3846,\n",
      "          21.5286,   1.1984,  -6.7886,  -1.0000,   4.2325,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         0.3730508 -1.         0.6339916]]\n",
      "State: tensor([[ -0.5206,  -1.0715,   3.8483,   0.7960,  -0.0513,   0.0397,\n",
      "           0.6018,  -3.5781,   1.9236,   0.6996,   8.1370,  12.8864,\n",
      "           6.1834,  -1.7843,   0.9959,   6.4663,  -0.0319,  -0.0587,\n",
      "           0.1495,   0.9865,  -3.2579,   1.8536,   4.1796,  -4.3349,\n",
      "          18.1743,  -1.7505,  -6.8172,  -1.0000,   4.1864,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.7190496   0.6189172  -1.          0.49935925]]\n",
      "State: tensor([[ -0.0030,   0.0406,   4.0307,   0.7053,   0.0000,   0.0006,\n",
      "           0.7089,  -3.3489,   1.3487,   0.0848,   5.4463,  13.4607,\n",
      "           0.9958,  -1.6911,   2.4727,   6.4165,  -0.1183,   0.0350,\n",
      "           0.2910,   0.9487,  -2.5141,   1.9284,   2.9854,  -3.3498,\n",
      "          14.6880,  -3.4059,  -6.8455,  -1.0000,   4.1400,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.64314127  0.57920146 -1.          0.08155715]]\n",
      "State: tensor([[  0.2683,   0.9993,   3.8964,   0.6146,   0.0207,  -0.0262,\n",
      "           0.7881,  -2.7764,   0.6294,  -0.1212,   2.5776,  10.9371,\n",
      "          -2.2500,  -1.6494,   3.7407,   6.1031,  -0.1710,   0.1054,\n",
      "           0.3687,   0.9076,  -1.5967,   1.2736,   1.5246,  -1.5494,\n",
      "          12.8379,  -5.0405,  -6.8734,  -1.0000,   4.0934,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.7717134   0.45036685 -1.         -0.22488445]]\n",
      "State: tensor([[  0.4091,   1.7631,   3.6047,   0.5307,   0.0271,  -0.0431,\n",
      "           0.8460,  -2.4332,   0.3448,  -0.1464,   1.5011,   8.9190,\n",
      "          -3.9425,  -1.6062,   4.8327,   5.5934,  -0.2025,   0.1511,\n",
      "           0.4053,   0.8786,  -1.1290,   0.8247,   0.7580,  -0.2842,\n",
      "          11.8246,  -7.1352,  -6.9011,  -1.0000,   4.0466,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.701553    0.30384827 -1.         -0.4197836 ]]\n",
      "State: tensor([[  0.5176,   2.4071,   3.1985,   0.4474,   0.0290,  -0.0576,\n",
      "           0.8920,  -2.3519,   0.2809,  -0.1897,   1.3539,   7.6921,\n",
      "          -5.3959,  -1.5274,   5.8060,   4.8964,  -0.2260,   0.1866,\n",
      "           0.4235,   0.8572,  -0.9880,   0.6633,   0.4243,   0.6434,\n",
      "          10.8961,  -9.4673,  -6.9284,  -1.0000,   3.9997,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.59696287  0.1490823  -1.         -0.60952604]]\n",
      "State: tensor([[  0.6233,   2.9384,   2.6996,   0.3635,   0.0284,  -0.0724,\n",
      "           0.9283,  -2.2526,   0.2481,  -0.2450,   1.3879,   6.2322,\n",
      "          -6.4474,  -1.3977,   6.6475,   4.0247,  -0.2468,   0.2166,\n",
      "           0.4288,   0.8416,  -0.9586,   0.4863,   0.1202,   1.6511,\n",
      "           9.4607, -11.6707,  -6.9554,  -1.0000,   3.9525,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.4813912   0.03322029 -1.         -0.7776392 ]]\n",
      "State: tensor([[  0.7403,   3.3475,   2.1378,   0.2805,   0.0261,  -0.0887,\n",
      "           0.9554,  -2.1568,   0.2318,  -0.3286,   1.5926,   4.6910,\n",
      "          -7.1452,  -1.2054,   7.3254,   3.0085,  -0.2695,   0.2440,\n",
      "           0.4222,   0.8304,  -1.0589,   0.3409,  -0.1315,   2.7133,\n",
      "           7.5579, -13.5695,  -6.9821,  -1.0000,   3.9052,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.47633398 -0.01629758 -1.         -0.85474503]]\n",
      "State: tensor([[  0.8808,   3.6360,   1.5293,   0.1976,   0.0220,  -0.1080,\n",
      "           0.9741,  -2.1225,   0.2108,  -0.4445,   1.9356,   3.2025,\n",
      "          -7.7235,  -0.9446,   7.8188,   1.8750,  -0.2961,   0.2718,\n",
      "           0.4062,   0.8206,  -1.2054,   0.2748,  -0.2941,   3.7122,\n",
      "           5.2911, -15.1285,  -7.0084,  -1.0000,   3.8577,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.45145768 -0.05428213 -1.         -0.9592947 ]]\n",
      "State: tensor([[  1.0502,   3.8025,   0.8834,   0.1135,   0.0152,  -0.1309,\n",
      "           0.9848,  -2.1339,   0.1599,  -0.5724,   2.3156,   1.6605,\n",
      "          -8.1683,  -0.6152,   8.1101,   0.6494,  -0.3255,   0.3002,\n",
      "           0.3830,   0.8107,  -1.3024,   0.2254,  -0.4120,   4.6610,\n",
      "           2.7273, -16.3091,  -7.0345,  -1.0000,   3.8101,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.3964026  -0.11180645 -1.         -0.95512974]]\n",
      "State: tensor([[  1.2490,   3.8391,   0.2149,   0.0278,   0.0046,  -0.1572,\n",
      "           0.9872,  -2.1731,   0.0701,  -0.6979,   2.6945,  -0.0026,\n",
      "          -8.3908,  -0.2200,   8.1748,  -0.6387,  -0.3567,   0.3285,\n",
      "           0.3539,   0.7997,  -1.3893,   0.1760,  -0.4974,   5.5450,\n",
      "          -0.2464, -17.0845,  -7.0602,  -1.0000,   3.7622,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.34529746 -0.21717781 -1.         -0.93911505]]\n",
      "State: tensor([[  1.4765,   3.7357,  -0.4540,  -0.0596,  -0.0112,  -0.1866,\n",
      "           0.9806,  -2.2345,  -0.0620,  -0.8107,   3.0623,  -1.7929,\n",
      "          -8.3030,   0.2377,   7.9847,  -1.9516,  -0.3901,   0.3564,\n",
      "           0.3193,   0.7867,  -1.4946,   0.1209,  -0.5626,   6.3807,\n",
      "          -3.6473, -17.3575,  -7.0855,  -1.0000,   3.7142,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.27341264 -0.3853945  -1.         -0.9600526 ]]\n",
      "State: tensor([[  1.7312,   3.4848,  -1.0922,  -0.1482,  -0.0333,  -0.2182,\n",
      "           0.9640,  -2.3102,  -0.2381,  -0.9009,   3.4137,  -3.6659,\n",
      "          -7.7851,   0.7549,   7.5135,  -3.2383,  -0.4270,   0.3844,\n",
      "           0.2781,   0.7698,  -1.6595,   0.0690,  -0.6263,   7.1792,\n",
      "          -7.4398, -16.9412,  -7.1106,  -1.0000,   3.6660,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.1928395 -0.5449926 -1.        -1.       ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State: tensor([[  2.0087,   3.0865,  -1.6607,  -0.2371,  -0.0632,  -0.2507,\n",
      "           0.9365,  -2.3988,  -0.4527,  -0.9436,   3.6915,  -5.4920,\n",
      "          -6.7494,   1.3233,   6.7450,  -4.4326,  -0.4688,   0.4140,\n",
      "           0.2282,   0.7461,  -1.9298,   0.0444,  -0.7149,   7.8241,\n",
      "         -11.4858, -15.6308,  -7.1353,  -1.0000,   3.6176,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.10897797 -0.62785995 -1.         -1.        ]]\n",
      "State: tensor([[  2.2983,   2.5563,  -2.1169,  -0.3249,  -0.1017,  -0.2817,\n",
      "           0.8971,  -2.5008,  -0.6809,  -0.9068,   3.8023,  -7.0581,\n",
      "          -5.1866,   1.9212,   5.6828,  -5.4566,  -0.5167,   0.4470,\n",
      "           0.1663,   0.7110,  -2.3669,   0.0859,  -0.8889,   8.1003,\n",
      "         -15.5097, -13.2665,  -7.1597,  -1.0000,   3.5691,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.03765512 -0.6938243  -1.         -1.        ]]\n",
      "State: tensor([[  2.5797,   1.9387,  -2.4203,  -0.4087,  -0.1481,  -0.3074,\n",
      "           0.8465,  -2.5536,  -0.8561,  -0.7554,   3.5901,  -7.9231,\n",
      "          -3.1572,   2.5076,   4.3739,  -6.2268,  -0.5706,   0.4851,\n",
      "           0.0865,   0.6570,  -3.0691,   0.2017,  -1.2840,   7.7193,\n",
      "         -18.8232,  -9.7862,  -7.1838,  -1.0000,   3.5204,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.08256197 -0.61304486 -1.         -1.        ]]\n",
      "State: tensor([[  2.8305,   1.2903,  -2.5624,  -0.4871,  -0.2003,  -0.3237,\n",
      "           0.7860,  -2.6534,  -0.9330,  -0.5279,   3.1274,  -8.2045,\n",
      "          -1.2196,   3.0299,   2.9109,  -6.6848,  -0.6228,   0.5270,\n",
      "          -0.0146,   0.5782,  -3.6827,   0.2890,  -1.8553,   6.6156,\n",
      "         -20.8286,  -5.4704,  -7.2075,  -1.0000,   3.4715,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.07219493 -0.5506067  -1.         -1.        ]]\n",
      "State: tensor([[  3.0451,   0.6094,  -2.5639,  -0.5659,  -0.2590,  -0.3262,\n",
      "           0.7115,  -3.0904,  -0.9469,  -0.2821,   2.6553,  -8.8072,\n",
      "           0.4757,   3.4545,   1.3568,  -6.8059,  -0.6556,   0.5741,\n",
      "          -0.1372,   0.4710,  -4.2189,   0.3662,  -2.8916,   5.0595,\n",
      "         -21.7890,  -0.6520,  -7.2309,  -1.0000,   3.4225,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.09183657 -0.4567011  -1.         -1.        ]]\n",
      "State: tensor([[  3.2000,  -0.1701,  -2.4355,  -0.6574,  -0.3257,  -0.3020,\n",
      "           0.6087,  -4.2954,  -0.6065,  -0.0063,   1.5023, -10.6588,\n",
      "           1.8301,   3.7601,  -0.2610,  -6.6307,  -0.6278,   0.6413,\n",
      "          -0.3127,   0.3112,  -4.7556,  -0.2058,  -4.9963,   3.9925,\n",
      "         -21.5566,   1.9727,  -7.2540,  -1.0000,   3.3733,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.02996147 -0.5222219  -1.         -1.        ]]\n",
      "State: tensor([[  3.1710,  -1.0765,  -2.2270,  -0.7631,  -0.3729,  -0.2315,\n",
      "           0.4744,  -4.8251,   0.4197,  -0.1596,  -1.0985, -11.5464,\n",
      "           2.8467,   3.9348,  -1.9519,  -6.4268,  -0.5404,   0.6697,\n",
      "          -0.4674,   0.2025,  -2.9950,  -1.3922,  -3.6854,   3.0663,\n",
      "         -22.0471,   1.6372,  -7.2767,  -1.0000,   3.3240,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.06707948 -0.52465546 -1.         -1.        ]]\n",
      "State: tensor([[  2.9191,  -1.9592,  -1.9443,  -0.8506,  -0.3660,  -0.1486,\n",
      "           0.3470,  -3.9567,   1.0972,  -0.9637,  -3.9181, -10.8868,\n",
      "           3.6920,   3.8716,  -3.6601,  -6.1284,  -0.4289,   0.6657,\n",
      "          -0.5953,   0.1360,  -2.3960,  -2.0333,  -3.0784,   0.4840,\n",
      "         -22.4702,   3.0451,  -7.2991,  -1.0000,   3.2745,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.10030878 -0.48011792 -1.         -1.        ]]\n",
      "State: tensor([[  2.4492,  -2.7332,  -1.6246,  -0.9137,  -0.3103,  -0.0839,\n",
      "           0.2488,  -2.7471,   1.1667,  -1.7682,  -6.5409,  -9.2569,\n",
      "           4.0539,   3.5167,  -5.2972,  -5.6996,  -0.3042,   0.6366,\n",
      "          -0.7023,   0.0946,  -2.1748,  -2.4183,  -2.5910,  -2.8573,\n",
      "         -21.9365,   5.2942,  -7.3212,  -1.0000,   3.2249,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.14120811 -0.40196466 -1.         -1.        ]]\n",
      "State: tensor([[  1.8050,  -3.3322,  -1.3107,  -0.9563,  -0.2266,  -0.0422,\n",
      "           0.1800,  -1.7919,   0.9522,  -2.2188,  -8.4677,  -6.8741,\n",
      "           3.8885,   2.8740,  -6.7612,  -5.1354,  -0.1796,   0.5829,\n",
      "          -0.7891,   0.0729,  -2.1901,  -2.5425,  -2.0003,  -6.3712,\n",
      "         -20.1430,   7.9400,  -7.3429,  -1.0000,   3.1751,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.19262612 -0.40613514 -1.         -1.        ]]\n",
      "State: tensor([[  1.0615,  -3.7214,  -1.0401,  -0.9820,  -0.1319,  -0.0177,\n",
      "           0.1339,  -1.1386,   0.7064,  -2.3523,  -9.3979,  -4.1992,\n",
      "           3.2881,   1.9913,  -7.9642,  -4.4477,  -0.0702,   0.5040,\n",
      "          -0.8583,   0.0668,  -2.4359,  -2.3376,  -1.3403,  -9.6309,\n",
      "         -17.1963,  10.6454,  -7.3643,  -1.0000,   3.1252,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.3398741  -0.47103202 -1.         -1.        ]]\n",
      "State: tensor([[  0.3062,  -3.9045,  -0.8374,  -0.9938,  -0.0379,  -0.0039,\n",
      "           0.1048,  -0.6737,   0.5103,  -2.2607,  -9.2585,  -1.7031,\n",
      "           2.3748,   0.9425,  -8.8607,  -3.6499,   0.0080,   0.3978,\n",
      "          -0.9148,   0.0699,  -2.9877,  -1.7088,  -0.7438, -12.3505,\n",
      "         -13.6589,  13.3963,  -7.3854,  -1.0000,   3.0751,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.4869544 -0.6342955 -1.        -1.       ]]\n",
      "State: tensor([[ -0.3753,  -3.9133,  -0.7576,  -0.9944,   0.0465,   0.0044,\n",
      "           0.0949,  -0.0531,   0.3780,  -1.9670,  -8.0473,   0.3883,\n",
      "           0.2919,  -0.1960,  -9.4647,  -2.7563,   0.0352,   0.2485,\n",
      "          -0.9655,   0.0700,  -4.3758,  -0.4002,  -0.4413, -14.6767,\n",
      "         -10.4269,  16.8199,  -7.4061,  -1.0000,   3.0249,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.51208264 -0.91811943 -1.         -0.71585053]]\n",
      "State: tensor([[ -0.8790,  -3.8046,  -0.8792,  -0.9876,   0.1093,   0.0123,\n",
      "           0.1120,   0.5886,   0.2918,  -1.3409,  -5.4151,   1.5376,\n",
      "          -2.0423,  -1.3408,  -9.6765,  -1.7465,  -0.0155,   0.0340,\n",
      "          -0.9987,   0.0343,  -5.8322,   1.5713,  -1.2913, -16.3993,\n",
      "          -2.6345,  21.3302,  -7.4265,  -1.0000,   2.9745,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.71913344 -1.         -1.          0.12860346]]\n",
      "State: tensor([[ -1.1887,  -3.7127,  -0.9052,  -0.9818,   0.1484,   0.0178,\n",
      "           0.1173,  -0.0637,   0.2200,  -0.8915,  -3.5457,   0.9404,\n",
      "           0.4854,  -2.3568,  -9.3197,  -0.5200,  -0.0755,  -0.1744,\n",
      "          -0.9800,  -0.0597,  -5.1712,   1.1230,  -2.3653, -15.1572,\n",
      "           8.0731,  23.5031,  -7.4465,  -1.0000,   2.9240,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.        -0.9772169 -1.         0.5875069]]\n",
      "State: tensor([[ -1.4578,  -3.6766,  -0.6211,  -0.9794,   0.1838,   0.0155,\n",
      "           0.0818,  -1.1826,   0.1783,  -0.9126,  -3.5096,   0.4115,\n",
      "           4.6281,  -3.2327,  -8.6106,   0.8922,  -0.0931,  -0.3390,\n",
      "          -0.9229,  -0.1571,  -4.2112,   0.2300,  -2.0411, -12.6515,\n",
      "          13.7024,  23.0032,  -7.4662,  -1.0000,   2.8734,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.9322783  -1.          0.71163654]]\n",
      "State: tensor([[ -1.7848,  -3.5871,  -0.0305,  -0.9737,   0.2279,   0.0013,\n",
      "           0.0043,  -2.2155,   0.0658,  -1.1889,  -4.3433,   1.5774,\n",
      "           8.1811,  -4.0167,  -7.7896,   2.3865,  -0.0951,  -0.4463,\n",
      "          -0.8615,  -0.2228,  -2.6848,   0.0965,  -1.3134, -10.4796,\n",
      "          13.8253,  21.0186,  -7.4855,  -1.0000,   2.8226,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.        -0.9485674 -1.         0.7176659]]\n",
      "State: tensor([[ -2.1129,  -3.3281,   0.7348,  -0.9564,   0.2716,  -0.0290,\n",
      "          -0.1034,  -2.9990,  -0.1757,  -1.1363,  -3.9938,   3.9131,\n",
      "           9.9362,  -4.7036,  -6.9035,   3.8953,  -0.0886,  -0.5028,\n",
      "          -0.8195,  -0.2605,  -1.4714,  -0.2013,  -0.6031,  -8.7835,\n",
      "          13.3661,  19.5826,  -7.5045,  -1.0000,   2.7717,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.        -1.        -1.         0.7366067]]\n",
      "State: tensor([[ -2.3698,  -2.8313,   1.5768,  -0.9207,   0.3002,  -0.0770,\n",
      "          -0.2372,  -3.8459,  -0.3719,  -0.8216,  -2.9730,   7.1425,\n",
      "          10.6837,  -5.2597,  -5.8831,   5.3780,  -0.0543,  -0.5241,\n",
      "          -0.8069,  -0.2669,  -0.6072,  -0.7199,   0.6663,  -7.0735,\n",
      "          13.7511,  18.3115,  -7.5231,  -1.0000,   2.7207,   0.0000,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.       -1.       -1.        0.765718]]\n",
      "State: tensor([[ -2.5001,  -2.0118,   2.4253,  -0.8557,   0.2985,  -0.1391,\n",
      "          -0.3992,  -4.8941,  -0.2650,  -0.2718,  -1.2053,  11.5329,\n",
      "          10.4589,  -5.6140,  -4.5880,   6.7502,   0.0395,  -0.5211,\n",
      "          -0.8217,  -0.2274,  -0.2643,  -1.6103,   2.6758,  -4.2211,\n",
      "          16.2786,  15.7213,  -7.5414,  -1.0000,   2.6696,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.        -0.5200535 -1.         0.7912321]]\n",
      "State: tensor([[ -2.4074,  -1.0316,   3.0554,  -0.7724,   0.2567,  -0.1835,\n",
      "          -0.5513,  -4.2795,   0.4520,   0.1996,   1.5824,  12.0463,\n",
      "           6.6486,  -5.5870,  -3.0074,   7.7172,   0.1588,  -0.5186,\n",
      "          -0.8206,  -0.1803,  -0.8527,  -1.5718,   2.1252,   0.8483,\n",
      "          19.7800,   9.9234,  -7.5594,  -1.0000,   2.6183,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.          0.09002614 -1.          0.642671  ]]\n",
      "State: tensor([[ -2.2155,  -0.2038,   3.3530,  -0.6993,   0.2099,  -0.1968,\n",
      "          -0.6544,  -2.9527,   0.6827,   0.0851,   2.2898,   9.5585,\n",
      "           2.7670,  -5.3109,  -1.4382,   8.2607,   0.2129,  -0.5457,\n",
      "          -0.7953,  -0.1562,  -1.3760,  -0.4225,   0.6762,   3.6184,\n",
      "          20.1171,   6.2107,  -7.5770,  -1.0000,   2.5669,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.          0.17153823 -1.          0.4733503 ]]\n",
      "State: tensor([[ -2.0235,   0.4525,   3.4502,  -0.6387,   0.1723,  -0.1956,\n",
      "          -0.7239,  -2.1948,   0.6505,  -0.0535,   2.2519,   7.6574,\n",
      "           0.7289,  -4.9548,  -0.0016,   8.5526,   0.2259,  -0.5936,\n",
      "          -0.7605,  -0.1352,  -1.6576,   0.1973,   0.1014,   4.7909,\n",
      "          19.2909,   3.8080,  -7.5943,  -1.0000,   2.5154,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.          0.13456178 -1.          0.36124074]]\n",
      "State: tensor([[ -1.8478,   0.9751,   3.4424,  -0.5880,   0.1431,  -0.1883,\n",
      "          -0.7735,  -1.6955,   0.5656,  -0.1370,   2.0661,   6.1098,\n",
      "          -0.3471,  -4.5676,   1.3077,   8.6716,   0.2182,  -0.6511,\n",
      "          -0.7181,  -0.1132,  -1.8592,   0.5050,  -0.2294,   5.4134,\n",
      "          18.3917,   1.7615,  -7.6112,  -1.0000,   2.4638,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.          0.03072083 -1.          0.2614467 ]]\n",
      "State: tensor([[ -1.6917,   1.3860,   3.3828,  -0.5461,   0.1204,  -0.1787,\n",
      "          -0.8095,  -1.3137,   0.4729,  -0.1771,   1.8335,   4.7731,\n",
      "          -0.8542,  -4.1655,   2.5155,   8.6547,   0.1969,  -0.7141,\n",
      "          -0.6655,  -0.0918,  -2.1105,   0.6609,  -0.5310,   5.8662,\n",
      "          17.8794,  -0.2031,  -7.6277,  -1.0000,   2.4121,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.08993143 -1.          0.22621262]]\n",
      "State: tensor([[ -1.5468,   1.7150,   3.3018,  -0.5110,   0.1022,  -0.1675,\n",
      "          -0.8369,  -1.0681,   0.4296,  -0.2094,   1.7666,   3.8806,\n",
      "          -1.0496,  -3.7415,   3.6584,   8.5034,   0.1656,  -0.7794,\n",
      "          -0.5998,  -0.0734,  -2.3651,   0.6666,  -0.8003,   6.4366,\n",
      "          17.7257,  -2.6175,  -7.6439,  -1.0000,   2.3602,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.18289775 -1.          0.18770945]]\n",
      "State: tensor([[ -1.3957,   1.9791,   3.2218,  -0.4824,   0.0864,  -0.1537,\n",
      "          -0.8580,  -0.8360,   0.4496,  -0.2634,   1.9570,   3.0873,\n",
      "          -0.9417,  -3.2750,   4.7630,   8.1954,   0.1269,  -0.8461,\n",
      "          -0.5140,  -0.0621,  -2.8051,   0.4975,  -1.1001,   7.2455,\n",
      "          17.7917,  -5.7973,  -7.6598,  -1.0000,   2.3082,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.31083655 -1.          0.10819495]]\n",
      "State: tensor([[ -1.2034,   2.1980,   3.1582,  -0.4595,   0.0703,  -0.1341,\n",
      "          -0.8752,  -0.6688,   0.5757,  -0.3857,   2.6464,   2.6071,\n",
      "          -0.6974,  -2.7342,   5.8047,   7.6916,   0.0886,  -0.9085,\n",
      "          -0.4039,  -0.0608,  -3.2524,   0.1591,  -1.1378,   8.2371,\n",
      "          16.7882,  -9.8434,  -7.6753,  -1.0000,   2.2562,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.4439634  -1.         -0.03681391]]\n",
      "State: tensor([[ -0.9415,   2.4134,   3.0910,  -0.4372,   0.0519,  -0.1059,\n",
      "          -0.8916,  -0.6866,   0.7352,  -0.5524,   3.5783,   2.6962,\n",
      "          -0.8588,  -2.0957,   6.7399,   6.9511,   0.0611,  -0.9581,\n",
      "          -0.2730,  -0.0607,  -3.5817,   0.0524,  -0.8301,   9.3578,\n",
      "          14.4945, -14.2998,  -7.6904,  -1.0000,   2.2040,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-1.         -0.46997613 -1.         -0.408912  ]]\n",
      "State: tensor([[ -0.6416,   2.6625,   2.9628,  -0.4076,   0.0326,  -0.0731,\n",
      "          -0.9097,  -0.9146,   0.7204,  -0.6177,   3.7548,   3.1906,\n",
      "          -1.8385,  -1.3689,   7.5342,   5.9497,   0.0354,  -0.9894,\n",
      "          -0.1319,  -0.0491,  -3.6586,   0.3531,  -0.8687,  10.7179,\n",
      "          11.2770, -18.4185,  -7.7052,  -1.0000,   2.1518,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.7516944  -0.37918985 -1.         -0.76580733]]\n",
      "State: tensor([[ -0.3790,   2.9547,   2.7238,  -0.3650,   0.0171,  -0.0440,\n",
      "          -0.9298,  -1.2500,   0.5126,  -0.5238,   2.9297,   3.7250,\n",
      "          -3.3466,  -0.5860,   8.1563,   4.6970,  -0.0121,  -0.9993,\n",
      "           0.0103,  -0.0328,  -3.5622,   0.3702,  -1.5888,  12.2880,\n",
      "           7.3450, -21.7285,  -7.7196,  -1.0000,   2.0994,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.49600375 -0.25295848 -1.         -1.        ]]\n",
      "State: tensor([[ -0.2014,   3.2923,   2.3312,  -0.3045,   0.0076,  -0.0239,\n",
      "          -0.9522,  -1.7328,   0.2659,  -0.3459,   1.7519,   4.3229,\n",
      "          -5.4529,   0.2305,   8.5664,   3.2153,  -0.0932,  -0.9841,\n",
      "           0.1472,  -0.0335,  -3.4241,  -0.2960,  -2.4319,  13.8346,\n",
      "           2.5805, -24.0881,  -7.7337,  -1.0000,   2.0469,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.30432922 -0.11091757 -1.         -1.        ]]\n",
      "State: tensor([[ -0.0822,   3.6590,   1.7182,  -0.2188,   0.0022,  -0.0100,\n",
      "          -0.9757,  -2.3900,   0.1464,  -0.2739,   1.2496,   4.5625,\n",
      "          -8.4631,   1.0727,   8.7070,   1.5239,  -0.1960,  -0.9394,\n",
      "           0.2736,  -0.0653,  -3.3734,  -1.2250,  -2.7311,  14.6926,\n",
      "          -3.0223, -25.2982,  -7.7474,  -1.0000,   1.9944,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.11536109 -0.0739975  -1.         -1.        ]]\n",
      "State: tensor([[  0.0583,   3.9617,   0.8288,  -0.1035,  -0.0009,   0.0072,\n",
      "          -0.9946,  -3.0875,   0.1230,  -0.4427,   1.8505,   3.3199,\n",
      "         -11.9833,   1.9377,   8.4903,  -0.3430,  -0.2973,  -0.8649,\n",
      "           0.3850,  -0.1240,  -3.3847,  -1.8417,  -2.4779,  14.5651,\n",
      "          -9.1620, -25.2785,  -7.7608,  -1.0000,   1.9417,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[-0.04367959 -0.09563643 -1.         -1.        ]]\n",
      "State: tensor([[  0.2941,   4.0295,  -0.3029,   0.0376,   0.0011,   0.0366,\n",
      "          -0.9986,  -3.6391,  -0.0008,  -0.8057,   3.2426,  -0.1963,\n",
      "         -14.6462,   2.8116,   7.8063,  -2.3105,  -0.3768,  -0.7706,\n",
      "           0.4768,  -0.1921,  -3.2893,  -1.8446,  -1.9238,  13.4079,\n",
      "         -15.4390, -24.3247,  -7.7738,  -1.0000,   1.8890,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.04515851 -0.06998324 -1.         -1.        ]]\n",
      "State: tensor([[  0.6364,   3.7074,  -1.5060,   0.1919,   0.0148,   0.0780,\n",
      "          -0.9782,  -3.9218,  -0.3425,  -1.0778,   4.5197,  -5.3367,\n",
      "         -14.7505,   3.6494,   6.5649,  -4.2382,  -0.4210,  -0.6745,\n",
      "           0.5522,  -0.2507,  -3.1146,  -1.1863,  -1.2867,  11.3022,\n",
      "         -21.7255, -22.5507,  -7.7864,  -1.0000,   1.8361,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.3234595  -0.04982269 -1.         -1.        ]]\n",
      "State: tensor([[  1.0449,   2.9804,  -2.5327,   0.3433,   0.0448,   0.1231,\n",
      "          -0.9301,  -3.9325,  -0.7848,  -1.0830,   5.2468, -10.0767,\n",
      "         -11.7493,   4.3854,   4.7718,  -5.9351,  -0.4203,  -0.5914,\n",
      "           0.6268,  -0.2841,  -2.9407,   0.1451,  -0.8559,   8.4491,\n",
      "         -27.4858, -19.7643,  -7.7987,  -1.0000,   1.7832,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.54111516  0.01015842 -1.         -1.        ]]\n",
      "State: tensor([[  1.4874,   1.9610,  -3.2072,   0.4819,   0.0918,   0.1643,\n",
      "          -0.8558,  -3.9115,  -1.2170,  -0.8787,   5.6920, -13.2408,\n",
      "          -6.9994,   4.9616,   2.5416,  -7.2389,  -0.3791,  -0.5302,\n",
      "           0.7079,  -0.2722,  -2.3414,   1.7048,  -0.9673,   4.8172,\n",
      "         -32.1093, -15.7455,  -7.8107,  -1.0000,   1.7302,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.6661364   0.17961788 -1.         -1.        ]]\n",
      "State: tensor([[  1.9608,   0.7377,  -3.4525,   0.6102,   0.1582,   0.1959,\n",
      "          -0.7512,  -4.2905,  -1.6146,  -0.4925,   6.0592, -15.6443,\n",
      "          -1.4983,   5.3084,   0.0155,  -8.0092,  -0.3062,  -0.4986,\n",
      "           0.7847,  -0.2046,  -1.3178,   3.0907,  -1.2462,  -0.2532,\n",
      "         -34.7201,  -9.6550,  -7.8222,  -1.0000,   1.6772,   0.0000,\n",
      "           1.0000,   0.0000,  -0.0973]])\n",
      "Action: [[ 0.7582185   0.24041963 -1.         -0.7299553 ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6579ca1dd6a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_done\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/30_Arbeit/2020_Udacity_ND_DeepReinforcementLearning/p2_continuouscontrol/agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/30_Arbeit/2020_Udacity_ND_DeepReinforcementLearning/p2_continuouscontrol/agent.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;31m# Get Q targets (for next states) from target model (on CPU)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0;31m# Compute Q targets for current states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mQ_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_targets_next\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/30_repo-GiEjasr4/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "num_epochs = 2\n",
    "tmax = 300\n",
    "EPS=0.1\n",
    "eps_decay=0.999\n",
    "min_eps=0.01\n",
    "BETA = 0.01\n",
    "beta_decay=0.995\n",
    "min_beta=0.001\n",
    "PRINT_EVERY=1\n",
    "\n",
    "# Keep track of success\n",
    "scores = []\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]\n",
    "    score = 0\n",
    "    \n",
    "    for t in range(tmax): \n",
    "        action = agent.act(state)\n",
    "        action = np.clip(action, agent.action_limits[0], agent.action_limits[1]).numpy()\n",
    "        print(f\"Action: {action}\")\n",
    "        env_info = env.step(action)[brain_name]\n",
    "        next_state = env_info.vector_observations[0]\n",
    "        reward = env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        \n",
    "        if done: \n",
    "            break\n",
    "            \n",
    "    scores.append(score)\n",
    "    scores_window.append(score)\n",
    "    \n",
    "    # display some progress every PRINT_EVERY iterations\n",
    "    if (e+1)% PRINT_EVERY ==0 :\n",
    "        print(\"\\rEpisode: {0:d}, score: {1:f}\".format(e+1,total_rewards),end=\"\")\n",
    "    \n",
    "    if np.mean(scores_window) >= 30: \n",
    "        print(\"\\nEnvironment solved in {:d} episodes!\\tAverage score: {:.2f}\".format\n",
    "             (epoch - 100, np.mean(scores_window)))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
